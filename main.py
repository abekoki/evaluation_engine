#!/usr/bin/env python3
"""
drowsy_detection è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³ ãƒ¡ã‚¤ãƒ³å®Ÿè£…

ä»•æ§˜æ›¸: docs/EVALUATION_SPEC.md ã«åŸºã¥ãè©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³
"""

import sys
import os
import json
import yaml
import pandas as pd
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# DataWareHouseãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
dwh_path = Path("../DataWareHouse")
if dwh_path.exists():
    sys.path.insert(0, str(dwh_path))

import datawarehouse as dwh
from drowsy_detection import DrowsyDetector, InputData, OutputData, Config
import drowsy_detection


class EvaluationEngine:
    """è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config = self._load_config(config_path)
        self.run_id = datetime.now().strftime("%Y%m%d-%H%M%S")
        self.db_path = os.path.abspath(self.config['datawarehouse']['database_path'])
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        self.output_base_dir = Path(self.config['output']['base_dir'])
        self.evaluation_dir = Path(self.config['output']['evaluation_dir'])
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æƒ…å ±ã®å–å¾—
        self.algorithm_commit_hash = self._get_algorithm_commit_hash()
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã§å‹•çš„ç”Ÿæˆ
        self.algorithm_version = self._get_dynamic_version(drowsy_detection.__version__, self.algorithm_commit_hash)
        
        print(f"[{self.run_id}] è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        print(f"  Database: {self.db_path}")
        print(f"  Algorithm version: {self.algorithm_version}")
        print(f"  Algorithm commit hash: {self.algorithm_commit_hash}")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            raise
    
    def _get_algorithm_commit_hash(self) -> str:
        """ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®Gitã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
        try:
            result = subprocess.run(
                ["git", "ls-remote", self.config['algorithm']['git_repo'], "HEAD"],
                capture_output=True,
                text=True,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip().split('\t')[0]
            else:
                return "unknown"
        except Exception:
            return "unknown"
    
    def _get_dynamic_version(self, base_version: str, commit_hash: str) -> str:
        """ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã§å‹•çš„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        if commit_hash == "unknown" or not commit_hash:
            return base_version
        
        # ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã®æœ€åˆã®8æ–‡å­—ã‚’ä½¿ç”¨
        short_hash = commit_hash[:8]
        
        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒ0.1.0ã§ã€æ–°ã—ã„ã‚³ãƒŸãƒƒãƒˆãŒã‚ã‚‹å ´åˆã¯0.1.1ã¨ã—ã¦æ‰±ã†
        if base_version == "0.1.0" and commit_hash != "e3803bb59fc690e096f82af4e7ba4aff235537ac":
            return f"0.1.1+{short_hash}"
        
        return f"{base_version}+{short_hash}"
    
    def run_evaluation(self) -> bool:
        """è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            print(f"\n[{self.run_id}] è©•ä¾¡é–‹å§‹")
            
            # 1. å®Ÿè¡Œæº–å‚™
            self._prepare_execution()
            
            # 2. å¯¾è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—
            core_outputs = self._get_target_data()
            
            # 3. æ¨è«–ã¨ã‚¢ãƒ«ã‚´CSVå‡ºåŠ›
            algorithm_results = self._run_algorithm_inference(core_outputs)
            
            # 4. è©•ä¾¡
            evaluation_results = self._run_evaluation_logic(algorithm_results)
            
            # 5. ãƒ­ã‚°å‡ºåŠ›
            self._write_log(evaluation_results)
            
            print(f"\n[{self.run_id}] è©•ä¾¡å®Œäº†")
            return True
            
        except Exception as e:
            print(f"\n[{self.run_id}] è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _prepare_execution(self):
        """å®Ÿè¡Œæº–å‚™"""
        print(f"[{self.run_id}] å®Ÿè¡Œæº–å‚™ä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        version_dir = self.output_base_dir / f"v{self.algorithm_version}"
        self.run_output_dir = version_dir / self.run_id
        self.run_output_dir.mkdir(parents=True, exist_ok=True)
        
        evaluation_version_dir = self.evaluation_dir / f"v{self.algorithm_version}"
        self.evaluation_output_dir = evaluation_version_dir / self.run_id
        self.evaluation_output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.run_output_dir}")
        print(f"  è©•ä¾¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.evaluation_output_dir}")
    
    def _get_target_data(self) -> List[Dict[str, Any]]:
        """å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—"""
        print(f"[{self.run_id}] å¯¾è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        
        try:
            core_outputs = dwh.list_core_lib_outputs(db_path=self.db_path)
            print(f"  å–å¾—ä»¶æ•°: {len(core_outputs)}")
            
            for output in core_outputs[:3]:  # æœ€åˆã®3ä»¶ã‚’è¡¨ç¤º
                print(f"    ID={output['core_lib_output_ID']}, "
                      f"video_ID={output['video_ID']}, "
                      f"dir={output['core_lib_output_dir']}")
            
            return core_outputs
            
        except Exception as e:
            print(f"  ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _run_algorithm_inference(self, core_outputs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ¨è«–ã¨ã‚¢ãƒ«ã‚´CSVå‡ºåŠ›"""
        print(f"[{self.run_id}] ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¨è«–å®Ÿè¡Œä¸­...")
        
        results = []
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç™»éŒ²
        try:
            algorithm_id = dwh.create_algorithm_version(
                version=self.algorithm_version,
                update_info=f"è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè¡Œ run_id={self.run_id}",
                commit_hash=self.algorithm_commit_hash,
                db_path=self.db_path
            )
            print(f"  ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç™»éŒ²: ID={algorithm_id}")
        except dwh.DWHUniqueConstraintError:
            # æ—¢å­˜ã®ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢
            existing = dwh.find_algorithm_by_commit_hash(self.algorithm_commit_hash, db_path=self.db_path)
            if existing:
                algorithm_id = existing['algorithm_ID']
                print(f"  æ—¢å­˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä½¿ç”¨: ID={algorithm_id}")
            else:
                print(f"  ã‚¨ãƒ©ãƒ¼: æ—¢å­˜ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
        
        for core_output in core_outputs:
            try:
                result = self._process_single_video(core_output, algorithm_id)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"  ãƒ“ãƒ‡ã‚ªå‡¦ç†ã‚¨ãƒ©ãƒ¼ (ID={core_output['video_ID']}): {e}")
                continue
        
        print(f"  å‡¦ç†å®Œäº†: {len(results)}ä»¶")
        return results
    
    def _process_single_video(self, core_output: Dict[str, Any], algorithm_id: int) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ãƒ“ãƒ‡ã‚ªã®å‡¦ç†"""
        video_id = core_output['video_ID']
        core_lib_output_id = core_output['core_lib_output_ID']
        
        print(f"    ãƒ“ãƒ‡ã‚ªID={video_id} å‡¦ç†ä¸­...")
        
        # ã‚³ã‚¢CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        core_dir = Path(self.db_path).parent / core_output['core_lib_output_dir']
        csv_files = list(core_dir.glob("*.csv"))
        
        if not csv_files:
            print(f"      CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {core_dir}")
            return None
        
        core_csv_path = csv_files[0]  # æœ€åˆã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        
        try:
            df = pd.read_csv(core_csv_path)
            print(f"      ã‚³ã‚¢CSVèª­ã¿è¾¼ã¿: {len(df)}è¡Œ")
        except Exception as e:
            print(f"      ã‚³ã‚¢CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œ
        try:
            config = Config()
            detector = DrowsyDetector(config)
            detector.set_frame_rate(float(self.config['algorithm']['frame_rate']))
            
            algo_results = []
            for _, row in df.iterrows():
                input_data = InputData(
                    frame_num=int(row['frame']),
                    left_eye_open=float(row['leye_openness']),
                    right_eye_open=float(row['reye_openness']),
                    face_confidence=float(row['confidence'])
                )
                
                output = detector.update(input_data)
                algo_results.append({
                    'frame_num': output.frame_num,
                    'is_drowsy': int(output.is_drowsy),
                    'left_eye_closed': output.left_eye_closed,
                    'right_eye_closed': output.right_eye_closed,
                    'continuous_time': output.continuous_time,
                    'error_code': output.error_code or ''
                })
            
            print(f"      ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œå®Œäº†: {len(algo_results)}ãƒ•ãƒ¬ãƒ¼ãƒ ")
            
        except Exception as e:
            print(f"      ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
        # ã‚¢ãƒ«ã‚´CSVã®ä¿å­˜
        try:
            algo_df = pd.DataFrame(algo_results)
            algo_csv_path = self.run_output_dir / f"{video_id}.csv"
            algo_df.to_csv(algo_csv_path, index=False)
            print(f"      ã‚¢ãƒ«ã‚´CSVä¿å­˜: {algo_csv_path}")
        except Exception as e:
            print(f"      ã‚¢ãƒ«ã‚´CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
        # DataWareHouseã«ç™»éŒ²
        try:
            # database.dbã‚’åŸºæº–ã¨ã—ãŸç›¸å¯¾ãƒ‘ã‚¹ã§DataWareHouseã«ç™»éŒ²
            db_dir = Path(self.db_path).parent.resolve()
            output_dir_relative = str(self.run_output_dir.resolve().relative_to(db_dir))
            algorithm_output_id = dwh.create_algorithm_output(
                algorithm_id=algorithm_id,
                core_lib_output_id=core_lib_output_id,
                output_dir=output_dir_relative,
                db_path=self.db_path
            )
            print(f"      DataWareHouseç™»éŒ²: algorithm_output_ID={algorithm_output_id}")
        except Exception as e:
            print(f"      DataWareHouseç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
        return {
            'video_id': video_id,
            'core_lib_output_id': core_lib_output_id,
            'algorithm_output_id': algorithm_output_id,
            'algo_csv_path': algo_csv_path,
            'algo_results': algo_results
        }
    
    def _run_evaluation_logic(self, algorithm_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ"""
        print(f"[{self.run_id}] è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œä¸­...")
        
        total_tasks = 0
        total_correct = 0
        per_video_results = []
        detailed_results = []  # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç”Ÿæˆç”¨ã®è©³ç´°çµæœ
        
        for result in algorithm_results:
            video_id = result['video_id']
            algo_results = result['algo_results']
            
            print(f"    ãƒ“ãƒ‡ã‚ªID={video_id} è©•ä¾¡ä¸­...")
            
            # ã‚¿ã‚°æƒ…å ±ã®å–å¾—
            try:
                tags = dwh.get_video_tags(video_id, db_path=self.db_path)
                print(f"      ã‚¿ã‚°æ•°: {len(tags)}")
            except Exception as e:
                print(f"      ã‚¿ã‚°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                continue
            
            # ãƒ“ãƒ‡ã‚ªã”ã¨ã®è©•ä¾¡
            video_tasks = []
            video_correct = 0
            
            for tag in tags:
                task_id = f"{video_id}_{tag['tag_ID']}"
                start_frame = tag['start']
                end_frame = tag['end']
                
                # åŒºé–“å†…ã®is_drowsyã‚’é›†è¨ˆ
                drowsy_frames = [
                    r for r in algo_results 
                    if start_frame <= r['frame_num'] <= end_frame and r['is_drowsy'] == 1
                ]
                
                predicted = 1 if len(drowsy_frames) > 0 else 0
                ground_truth = 1  # ä»•æ§˜æ›¸ã«åŸºã¥ãã€å…¨ã‚¿ã‚°åŒºé–“ãŒã€Œé€£ç¶šé–‰çœ¼ã‚ã‚Šã€
                correct = int(predicted == ground_truth)
                
                video_tasks.append({
                    'task_id': task_id,
                    'predicted': predicted,
                    'ground_truth': ground_truth,
                    'correct': correct,
                    'notes': f"frames {start_frame}-{end_frame}, drowsy_frames={len(drowsy_frames)}"
                })
                
                video_correct += correct
            
            # ãƒ“ãƒ‡ã‚ªã”ã¨ã®çµæœä¿å­˜
            if video_tasks:
                video_accuracy = video_correct / len(video_tasks)
                video_result = {
                    'video_id': video_id,
                    'accuracy': video_accuracy,
                    'num_correct': video_correct,
                    'num_tasks': len(video_tasks),
                    'result_file_path': f"{video_id}.csv"
                }
                per_video_results.append(video_result)
                
                # è©³ç´°çµæœã®ä¿å­˜
                tasks_df = pd.DataFrame(video_tasks)
                csv_path = self.evaluation_output_dir / f"{video_id}.csv"
                tasks_df.to_csv(csv_path, index=False)
                print(f"      è©•ä¾¡çµæœä¿å­˜: {csv_path}")
                print(f"      æ­£è§£ç‡: {video_accuracy:.3f} ({video_correct}/{len(video_tasks)})")
                
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç”Ÿæˆç”¨ã®è©³ç´°çµæœã‚’è¿½åŠ 
                detailed_results.append({
                    'video_id': video_id,
                    'algorithm_results': algo_results,
                    'evaluation_result': video_tasks
                })
                
                total_tasks += len(video_tasks)
                total_correct += video_correct
        
        # å…¨ä½“ã‚µãƒãƒªã®ä½œæˆ
        overall_accuracy = total_correct / total_tasks if total_tasks > 0 else 0.0
        
        evaluation_summary = {
            'evaluation_summary': {
                'run_id': self.run_id,
                'created_at': datetime.now().isoformat(),
                'algorithm_version': self.algorithm_version,
                'algorithm_commit_hash': self.algorithm_commit_hash,
                'evaluation_conditions': {
                    'frame_rate': self.config['algorithm']['frame_rate'],
                    'ground_truth': 'all_tags_continuous_closed_eyes'
                },
                'overall_results': {
                    'accuracy': overall_accuracy,
                    'total_num_correct': total_correct,
                    'total_num_tasks': total_tasks
                },
                'per_dataset': per_video_results
            }
        }
        
        # ã‚µãƒãƒªã®ä¿å­˜
        summary_path = self.evaluation_output_dir / "evaluation_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_summary, f, ensure_ascii=False, indent=2)
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        markdown_path = self._generate_markdown_report(evaluation_summary, detailed_results)
        
        print(f"    å…¨ä½“æ­£è§£ç‡: {overall_accuracy:.3f} ({total_correct}/{total_tasks})")
        print(f"    è©•ä¾¡ã‚µãƒãƒªä¿å­˜: {summary_path}")
        print(f"    ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {markdown_path}")
        
        return evaluation_summary
    
    def _generate_markdown_report(self, evaluation_summary: Dict[str, Any], evaluation_results: List[Dict[str, Any]]) -> str:
        """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        from datetime import datetime
        
        summary = evaluation_summary['evaluation_summary']
        markdown_lines = []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        markdown_lines.append("# drowsy_detection è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        markdown_lines.append("")
        markdown_lines.append(f"**å®Ÿè¡Œæ—¥æ™‚**: {summary['created_at']}")
        markdown_lines.append(f"**å®Ÿè¡ŒID**: `{summary['run_id']}`")
        markdown_lines.append(f"**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: `{summary['algorithm_version']}`")
        markdown_lines.append(f"**ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥**: `{summary['algorithm_commit_hash']}`")
        markdown_lines.append("")
        
        # è©•ä¾¡æ¡ä»¶
        markdown_lines.append("## ğŸ“‹ è©•ä¾¡æ¡ä»¶")
        markdown_lines.append("")
        conditions = summary['evaluation_conditions']
        markdown_lines.append(f"- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ**: {conditions['frame_rate']} fps")
        markdown_lines.append(f"- **ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹**: {conditions['ground_truth']}")
        markdown_lines.append("")
        
        # å…¨ä½“çµæœ
        overall = summary['overall_results']
        accuracy_percent = overall['accuracy'] * 100
        markdown_lines.append("## ğŸ¯ å…¨ä½“è©•ä¾¡çµæœ")
        markdown_lines.append("")
        markdown_lines.append(f"- **å…¨ä½“æ­£è§£ç‡**: **{accuracy_percent:.1f}%** ({overall['total_num_correct']}/{overall['total_num_tasks']})")
        
        # æ­£è§£ç‡ã®è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆ
        if accuracy_percent >= 100:
            status_emoji = "ğŸŸ¢"
            status_text = "OK"
        else:
            status_emoji = "ğŸ”´"
            status_text = "æœªæ¤œçŸ¥ã‚ã‚Š"
        
        markdown_lines.append(f"- **è©•ä¾¡**: {status_emoji} {status_text}")
        markdown_lines.append("")
        
        # ãƒ“ãƒ‡ã‚ªåˆ¥çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        markdown_lines.append("## ğŸ“Š ãƒ“ãƒ‡ã‚ªåˆ¥è©•ä¾¡çµæœ")
        markdown_lines.append("")
        markdown_lines.append("| ãƒ“ãƒ‡ã‚ªID | æ­£è§£ç‡ | æ­£è§£æ•°/ç·æ•° | ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ | è©³ç´° |")
        markdown_lines.append("|---------|--------|------------|-----------|------|")
        
        for dataset in summary['per_dataset']:
            video_id = dataset['video_id']
            accuracy = dataset['accuracy'] * 100
            correct = dataset['num_correct']
            total = dataset['num_tasks']
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ±ºå®š
            if accuracy >= 80:
                status = "ğŸŸ¢ å„ªç§€"
            elif accuracy >= 60:
                status = "ğŸŸ¡ è‰¯å¥½"
            elif accuracy >= 40:
                status = "ğŸŸ  è¦æ”¹å–„"
            else:
                status = "ğŸ”´ è¦æ”¹å–„"
            
            markdown_lines.append(f"| {video_id} | {accuracy:.1f}% | {correct}/{total} | {status} | [è©³ç´°](#ãƒ“ãƒ‡ã‚ª{video_id}ã®è©³ç´°çµæœ) |")
        
        markdown_lines.append("")
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¤œå‡ºçµ±è¨ˆ
        markdown_lines.append("## ğŸ” ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¤œå‡ºçµ±è¨ˆ")
        markdown_lines.append("")
        markdown_lines.append("| ãƒ“ãƒ‡ã‚ªID | ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•° | æ¤œå‡ºãƒ•ãƒ¬ãƒ¼ãƒ æ•° | æ¤œå‡ºç‡ | ã‚°ãƒ©ãƒ• |")
        markdown_lines.append("|---------|-------------|---------------|--------|-------|")
        
        # å„ãƒ“ãƒ‡ã‚ªã®æ¤œå‡ºçµ±è¨ˆã‚’å–å¾—
        for result in evaluation_results:
            video_id = result['video_id']
            algorithm_results = result['algorithm_results']
            total_frames = len(algorithm_results)
            drowsy_frames = sum(1 for r in algorithm_results if r['is_drowsy'])
            detection_rate = (drowsy_frames / total_frames) * 100 if total_frames > 0 else 0
            
            # ç°¡æ˜“ã‚°ãƒ©ãƒ•ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼é¢¨ï¼‰
            bar_length = 20
            filled_length = int(bar_length * (detection_rate / 100))
            bar = "â–ˆ" * filled_length + "â–‘" * (bar_length - filled_length)
            
            markdown_lines.append(f"| {video_id} | {total_frames:,} | {drowsy_frames:,} | {detection_rate:.2f}% | `{bar}` |")
        
        markdown_lines.append("")
        
        # è©³ç´°çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³
        markdown_lines.append("## ğŸ“– è©³ç´°çµæœ")
        markdown_lines.append("")
        
        for result in evaluation_results:
            video_id = result['video_id']
            evaluation_result = result['evaluation_result']
            
            markdown_lines.append(f"### ãƒ“ãƒ‡ã‚ª{video_id}ã®è©³ç´°çµæœ")
            markdown_lines.append("")
            
            # ã‚¿ã‚°åˆ¥è©•ä¾¡çµæœ
            markdown_lines.append("| ã‚¿ã‚¹ã‚¯ID | ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å‡ºåŠ› | çœŸå€¤ | æ­£è§£ | è©³ç´° |")
            markdown_lines.append("|----------|----------|------|------|------|")
            
            for eval_result in evaluation_result:
                task_id = eval_result['task_id']
                predicted = "âœ… æ¤œå‡º" if eval_result['predicted'] else "âŒ æœªæ¤œå‡º"
                ground_truth = "âœ… æ¤œå‡º" if eval_result['ground_truth'] else "âŒ æœªæ¤œå‡º"
                correct = "âœ…" if eval_result['correct'] else "âŒ"
                notes = eval_result.get('notes', '')
                
                markdown_lines.append(f"| {task_id} | {predicted} | {ground_truth} | {correct} | {notes} |")
            
            markdown_lines.append("")
        
        # ãƒ•ãƒƒã‚¿ãƒ¼
        markdown_lines.append("---")
        markdown_lines.append("*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        markdown_content = "\n".join(markdown_lines)
        markdown_path = self.evaluation_output_dir / "evaluation_report.md"
        
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return str(markdown_path)
    
    def _write_log(self, evaluation_results: Dict[str, Any]):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°"""
        log_file = self.config['logging']['file']
        
        overall = evaluation_results['evaluation_summary']['overall_results']
        log_entry = f"""
## è©•ä¾¡å®Ÿè¡Œãƒ­ã‚° - {self.run_id}

- **å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **å¯¾è±¡ä»¶æ•°**: {len(evaluation_results['evaluation_summary']['per_dataset'])}å‹•ç”»
- **å…¨ä½“æ­£è§£ç‡**: {overall['accuracy']:.3f} ({overall['total_num_correct']}/{overall['total_num_tasks']})
- **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: {self.algorithm_version}
- **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒãƒƒã‚·ãƒ¥**: {self.algorithm_commit_hash}
- **å‡ºåŠ›å…ˆ**: 
  - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å‡ºåŠ›: `{self.run_output_dir}`
  - è©•ä¾¡çµæœ: `{self.evaluation_output_dir}`

"""
        
        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
            print(f"[{self.run_id}] ãƒ­ã‚°æ›´æ–°: {log_file}")
        except Exception as e:
            print(f"[{self.run_id}] ãƒ­ã‚°æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("drowsy_detection è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³")
    print("=" * 50)
    
    try:
        engine = EvaluationEngine()
        success = engine.run_evaluation()
        exit(0 if success else 1)
    except Exception as e:
        print(f"è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()
